{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Loading and Checking the Dataset\n",
    "\n",
    "In this section, we will load the dataset and perform an initial check to understand its structure and contents. This includes importing necessary libraries, creating a sample dataframe, and displaying the dataframe to verify the data.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from dateparser import parse\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pancoran Mas' 'Sawangan' 'Citayam' 'Bojongsari' 'Limo' 'Cipayung'\n",
      " 'Senen' 'Cibinong' 'Beji' 'Sukmajaya' 'Tapos' 'Cimanggis' 'Cibubur'\n",
      " 'Jagakarsa' 'Ciracas' 'Pasar Rebo' 'Cilangkap' 'Gunung Putri'\n",
      " 'Jati Sampurna' 'Kalisari' 'Cilodong' 'Bojonggede' 'Pamulang'\n",
      " 'Ciputat Timur' 'Cinere' 'Gunung Sindur' 'Parung' 'Cisauk' 'Tajurhalang'\n",
      " 'Kemang' 'Bekasi Barat' 'Medan Satria' 'Bekasi Jaya' 'Bekasi Timur'\n",
      " 'Bekasi Utara' 'Cakung' 'Tarumajaya' 'Pondok Gede' 'Jatiasih' 'Jatiwarna'\n",
      " 'Pondok Melati' 'Bambu Apus' 'Bekasi Selatan' 'Bantar Gebang'\n",
      " 'Jatimakmur' 'Rawalumbu' 'Jatimulya' 'Mustika Jaya' 'Cileungsi'\n",
      " 'Duren Sawit' 'Makasar' 'Sawah Besar' 'Tambun Selatan' 'Cikarang Barat'\n",
      " 'Setu' nan 'Bogor Barat - Kota' 'Tanah Sareal' 'Bogor Utara - Kota'\n",
      " 'Ciomas' 'Bogor Timur - Kota' 'Tanah Sereal' 'Bumi Serpong Damai'\n",
      " 'Bogor Tengah - Kota' 'Cikarang Selatan' 'Parung Panjang' 'Ciputat'\n",
      " 'Dramaga' 'Sukaraja' 'Bogor Selatan - Kota' 'Cijeruk' 'Tamansari'\n",
      " 'Babakan Madang' 'Megamendung' 'Pasar Minggu' 'Tanjung Barat' 'Cilandak'\n",
      " 'Kebayoran Lama' 'Mampang Prapatan' 'Kebayoran Baru' 'Pesanggrahan'\n",
      " 'Larangan' 'Pondok Aren' 'Kramat Jati' 'Pancoran' 'Setia Budi'\n",
      " 'Tanah Abang' 'Tebet' 'Kebon Jeruk' 'Menteng' 'Kembangan' 'Palmerah'\n",
      " 'Johar Baru' 'Gambir' 'Matraman' 'Jatinegara' 'Ranca Bungur' 'Ciseeng'\n",
      " 'Ciampea' 'Pasar Baru' 'Penjaringan' 'Tanjung Duren' 'Grogol Petamburan'\n",
      " 'Tomang' 'Tambora' 'Taman Sari' 'Kalideres' 'Cengkareng' 'Daan Mogot'\n",
      " 'Pantai Indah Kapuk' 'Karang Tengah' 'Batuceper' 'Benda' 'Pulo Gadung'\n",
      " 'Kelapa Gading' 'Pekayon' 'Cempaka Putih' 'Kemayoran' 'Tanjung Priok'\n",
      " 'Koja' 'Pademangan' 'Tenjo' 'Kepulauan Seribu Utara' 'Pakuhaji'\n",
      " 'Teluknaga' 'Kosambi' 'Mauk' 'Sukadiri' 'Sepatan Timur' 'Rajeg' 'Sepatan'\n",
      " 'Serpong' 'Serpong Utara' 'Ciledug' 'Pagedangan' 'Bojongsoang'\n",
      " 'Gading Serpong' 'Pinang (Penang)' 'Kelapa Dua' 'Curug' 'Cibodas' 'Legok'\n",
      " 'Tangerang' 'Karawaci' 'Periuk' 'Jatiuwung' 'Neglasari' 'Cipondoh'\n",
      " 'Gubeng' 'Sukolilo' 'Raya Darmo' 'Genteng' 'Mulyorejo' 'Tegalsari'\n",
      " 'Kenjeran' 'Bulak' 'Tambaksari' 'Simokerto' 'Tenggilis Mejoyo' 'Waru'\n",
      " 'Gayungan' 'Dukuh Pakis' 'Rungkut' 'Wonokromo' 'Gununganyar' 'Semampir'\n",
      " 'Sukomanunggal' 'Sawahan' 'Wonocolo' 'Graha' 'Sedati' 'Mayjen Sungkono'\n",
      " 'Tidar' 'Bubutan' 'Tandes' 'Lakarsantri' 'Wiyung' 'Citraland' 'Pakuwon'\n",
      " 'Sambikerep' 'Benowo' 'Taman' 'Menganti' 'Kebomas' 'Driyorejo'\n",
      " 'Karangpilang' 'Pakal' 'Jambangan' 'Cerme' 'HR Muhammad' 'Gedangan'\n",
      " 'Sukodono' 'Krembangan' 'Gresik']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Data\n",
    "df = pd.read_csv(r'E:\\Programming\\Python\\Proyek Scraping\\olx\\rumah\\jabodetabeksur_olx_housing_dataset.csv')\n",
    "\n",
    "# Menampilkan daftar unik pada kolom 'address_district'\n",
    "unique_districts = df['address_district'].unique()\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(unique_districts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Fixing Numerical Type Columns\n",
    "\n",
    "This section address and correcting the data types of several numerical columns in our dataset. These columns include:\n",
    "\n",
    "- `bedrooms`\n",
    "- `bathrooms`\n",
    "- `maid_bedrooms`\n",
    "- `maid_bathrooms`\n",
    "- `ruang_tamu`\n",
    "- `ruang_makan`\n",
    "- `additional_rooms`\n",
    "- `floors`\n",
    "\n",
    "Ensuring these column's value makes sense while also changing their type to float.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['type'] == 'Apartemen', 'floors'] = 1.0\n",
    "\n",
    "df['maid_bathrooms'] = df['maid_bathrooms'].astype(float)\n",
    "df['maid_bedrooms'] = df['maid_bedrooms'].astype(float)\n",
    "df['ruang_makan'] = df['ruang_makan'].astype(float)\n",
    "df['ruang_tamu'] = df['ruang_tamu'].astype(float)\n",
    "df['additional_rooms'] = df['additional_rooms'].fillna(0).astype(int)\n",
    "df['floors'] = df['floors'].astype(float)\n",
    "\n",
    "output_file = \"step1_jabodetabeksur_olx_housing_dataset_.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Markdown\n",
    "# First Step of Exctracting Information\n",
    "\n",
    "In this step the goal is to extract information from description and facilities column to fill and/or fix missing value in the floors, additional_rooms, certificate, garage_capacity, carport_capacity, electricity_capacity, and house_orientation column.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 16148/16148 [00:05<00:00, 2890.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved to updated_jabodetabeksur_olx_housing_dataset_.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to extract information from descriptions\n",
    "def scrape_description(ads_description, kata_kunci_list, entity_type=None):\n",
    "    headings = [\n",
    "        \"Timur\", \"Tenggara\", \"Selatan\", \"Barat Daya\", \"Barat\", \"Barat Laut\", \n",
    "        \"Utara\", \"Timur Laut\"\n",
    "    ]\n",
    "    \n",
    "    if not isinstance(ads_description, list):\n",
    "        return None\n",
    "    \n",
    "    for kata_kunci in kata_kunci_list:\n",
    "        if entity_type == 'electricity':\n",
    "            pattern = rf'{re.escape(kata_kunci)}\\s*[\\:\\.\\-\\s]*([\\d.,]+)\\s*(watt|va|kva|token|w|wt|kwh)?'\n",
    "        elif entity_type in ['garage', 'carport']:\n",
    "            pattern = rf'{re.escape(kata_kunci)}\\s*[\\:\\.\\-\\s]*(\\d+)?\\s*(mobil|mbl|cars?)?'\n",
    "        elif entity_type == 'heading':\n",
    "            pattern = rf'{re.escape(kata_kunci)}\\s*[\\:\\.\\-\\s]*(.*?)(Timur|Tenggara|Selatan|Barat Daya|Barat Laut|Barat|Utara|Timur Laut|Kiblat|Khiblat)'\n",
    "        elif entity_type == 'ownership':\n",
    "            pattern = r'\\b(SHM|Sertifikat Hak Milik|HGB|Hak Guna Bangunan)\\b'\n",
    "        else:\n",
    "            pattern = rf'{re.escape(kata_kunci)}\\s*[\\:\\.\\-\\s]*([\\w\\s\\.,]+)'\n",
    "\n",
    "        for sentence in ads_description:\n",
    "            if isinstance(sentence, str) and kata_kunci.lower() in sentence.lower():\n",
    "                match = re.search(pattern, sentence, re.IGNORECASE)\n",
    "                if match:\n",
    "                    if entity_type == 'electricity':\n",
    "                        value_str = match.group(1).replace('.', '').replace(',', '')\n",
    "                        return int(value_str)\n",
    "                    elif entity_type in ['garage', 'carport']:\n",
    "                        value_str = match.group(1)\n",
    "                        return int(value_str) if value_str else 1\n",
    "                    elif entity_type == 'heading':\n",
    "                        heading = match.group(2).capitalize()\n",
    "                        return 'Barat' if heading.lower() in ['kiblat', 'khiblat'] else heading\n",
    "                    elif entity_type == 'ownership':\n",
    "                        ownership = match.group(1).lower()\n",
    "                        return \"SHM\" if \"shm\" in ownership or \"sertifikat hak milik\" in ownership else \"HGB\" if \"hgb\" in ownership or \"hak guna bangunan\" in ownership else None\n",
    "                    else:\n",
    "                        value_str = match.group(1).strip().split(\" \")[0]\n",
    "                        return value_str\n",
    "    return None\n",
    "\n",
    "# Function to count additional rooms\n",
    "def count_rooms(ads_description, keyword_list):\n",
    "    for keyword in keyword_list:\n",
    "        pattern = rf'(\\d+)?\\s*{re.escape(keyword)}'\n",
    "        for sentence in ads_description:\n",
    "            match = re.search(pattern, sentence, re.IGNORECASE)\n",
    "            if match:\n",
    "                return int(match.group(1)) if match.group(1) else 1\n",
    "    return 0\n",
    "\n",
    "# Function to extract number of floors\n",
    "def extract_floors(ads_description, keyword_list):\n",
    "    for keyword in keyword_list:\n",
    "        pattern = rf'(\\d+[,.]?\\d*)\\s*{re.escape(keyword)}|{re.escape(keyword)}\\s*[\\:\\-\\s]*(\\d+[,.]?\\d*)'\n",
    "        for sentence in ads_description:\n",
    "            match = re.search(pattern, sentence, re.IGNORECASE)\n",
    "            if match:\n",
    "                number = match.group(1) if match.group(1) else match.group(2)\n",
    "                return float(number.replace(',', '.')) if number else 1\n",
    "    return 1\n",
    "\n",
    "# Convert floors column to float\n",
    "df['floors'] = df['floors'].astype(float)\n",
    "\n",
    "# Iterate over dataset and update only missing values\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n",
    "    description = eval(row['description']) if isinstance(row['description'], str) else row['description']\n",
    "    property_type = row['type']\n",
    "    \n",
    "    facilities = row['facilities']\n",
    "\n",
    "    if not isinstance(description, list):\n",
    "        description = [description] if isinstance(description, str) else []\n",
    "    \n",
    "    # Convert facilities column to a list if it's not already\n",
    "    if isinstance(facilities, str):\n",
    "        try:\n",
    "            facilities = eval(facilities)  # Convert string representation of list to an actual list\n",
    "        except:\n",
    "            facilities = [facilities]\n",
    "    elif not isinstance(facilities, list):\n",
    "        facilities = []\n",
    "\n",
    "    # Update electricity capacity if missing\n",
    "    if pd.isna(row['electricity_capacity']) or row['electricity_capacity'] == \"\":\n",
    "        df.at[index, 'electricity_capacity'] = scrape_description(description, ['Listrik', 'Daya Listrik'], entity_type='electricity')\n",
    "\n",
    "    # Process garage capacity\n",
    "    if row['garage_capacity'] == 0:\n",
    "        garage_capacity = scrape_description(description, ['Garasi'], entity_type='garage')\n",
    "        if garage_capacity is None and \"Garasi\" in facilities:\n",
    "            garage_capacity = 1  # If \"Garasi\" is found in facilities, set to 1\n",
    "        df.at[index, 'garage_capacity'] = garage_capacity if garage_capacity is not None else 0  # Keep default if no data found\n",
    "\n",
    "    # Process carport capacity\n",
    "    if row['carport_capacity'] == 0:\n",
    "        carport_capacity = scrape_description(description, ['Carport'], entity_type='carport')\n",
    "        if carport_capacity is None and \"Carport\" in facilities:\n",
    "            carport_capacity = 1  # If \"Carport\" is found in facilities, set to 1\n",
    "        df.at[index, 'carport_capacity'] = carport_capacity if carport_capacity is not None else 0  # Keep default if no data found\n",
    "\n",
    "    # Update house orientation if missing\n",
    "    if pd.isna(row['house_orientation']) or row['house_orientation'] == \"\":\n",
    "        df.at[index, 'house_orientation'] = scrape_description(description, ['Arah', 'Orientasi', 'Hadap'], entity_type='heading')\n",
    "    \n",
    "    # Update ownership type if missing\n",
    "    if pd.isna(row['certificate']) or row['certificate'] == \"\":\n",
    "        df.at[index, 'certificate'] = scrape_description(description, ['Hak Milik', 'Sertifikat', 'Hak', 'SHM', 'HGB'], entity_type='ownership')\n",
    "\n",
    "    # Update additional rooms if missing\n",
    "    if row['additional_rooms'] == 0.0:\n",
    "        df.at[index, 'additional_rooms'] = 1 if count_rooms(description, ['Ruang', 'Kamar', 'Dapur', 'Gudang']) > 0 else 0\n",
    "\n",
    "    if row['floors'] == 0.0:\n",
    "        df.at[index, 'floors'] = 1 if property_type == \"Apartemen\" else extract_floors(description, ['Lantai'])\n",
    "\n",
    "# Save the updated DataFrame\n",
    "output_file = \"updated_jabodetabeksur_olx_housing_dataset_.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Updated CSV saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Markdown\n",
    "# Second Step of Extracting Information\n",
    "\n",
    "This step going to extract information from the description again to find a more relevant information for the profile matching step\n",
    "\n",
    "## List of Information Needed\n",
    "1. Is it free from flooding?\n",
    "2. Is it near public transportation (LRT/MRT/TJ)\n",
    "3. Is it near highway? (Car oriented development)\n",
    "4. Is it near any educational facilities? (TK/SD/SMP/SMA)\n",
    "5. Is it near or in the heart of the city? (Might need some visualisation of the given district to determine this)\n",
    "\n",
    "## List of Possible New Column\n",
    "1. TK\n",
    "2. SD\n",
    "3. SMP\n",
    "4. SMP\n",
    "5. UNIV\n",
    "6. Public Transport\n",
    "7. Pusat Perbelajaan\n",
    "8. Rumah Sakit\n",
    "\n",
    "## Assign a Strategic Score \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16148/16148 [13:26:41<00:00,  3.00s/it]  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Columns must be same length as key",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Apply geocoding to get lat/lon for each unique subdistrict\u001b[39;00m\n\u001b[32m     20\u001b[39m tqdm.pandas()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlatitude\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlongitude\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m = df[\u001b[33m'\u001b[39m\u001b[33maddress_subdistrict\u001b[39m\u001b[33m'\u001b[39m].progress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: pd.Series(get_coordinates(x)))\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Drop rows where coordinates could not be found\u001b[39;00m\n\u001b[32m     24\u001b[39m df.dropna(subset=[\u001b[33m'\u001b[39m\u001b[33mlatitude\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlongitude\u001b[39m\u001b[33m'\u001b[39m], inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:4299\u001b[39m, in \u001b[36mDataFrame.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4297\u001b[39m     \u001b[38;5;28mself\u001b[39m._setitem_frame(key, value)\n\u001b[32m   4298\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (Series, np.ndarray, \u001b[38;5;28mlist\u001b[39m, Index)):\n\u001b[32m-> \u001b[39m\u001b[32m4299\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setitem_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4300\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[32m   4301\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_item_frame_value(key, value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:4341\u001b[39m, in \u001b[36mDataFrame._setitem_array\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4336\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4337\u001b[39m     \u001b[38;5;66;03m# Note: unlike self.iloc[:, indexer] = value, this will\u001b[39;00m\n\u001b[32m   4338\u001b[39m     \u001b[38;5;66;03m#  never try to overwrite values inplace\u001b[39;00m\n\u001b[32m   4340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[32m-> \u001b[39m\u001b[32m4341\u001b[39m         \u001b[43mcheck_key_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4342\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k1, k2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(key, value.columns):\n\u001b[32m   4343\u001b[39m             \u001b[38;5;28mself\u001b[39m[k1] = value[k2]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexers\\utils.py:390\u001b[39m, in \u001b[36mcheck_key_length\u001b[39m\u001b[34m(columns, key, value)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m columns.is_unique:\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value.columns) != \u001b[38;5;28mlen\u001b[39m(key):\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mColumns must be same length as key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    392\u001b[39m     \u001b[38;5;66;03m# Missing keys in columns are represented as -1\u001b[39;00m\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns.get_indexer_non_unique(key)[\u001b[32m0\u001b[39m]) != \u001b[38;5;28mlen\u001b[39m(value.columns):\n",
      "\u001b[31mValueError\u001b[39m: Columns must be same length as key"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from geopy.geocoders import Nominatim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize geolocator\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "\n",
    "# Function to get latitude & longitude\n",
    "def get_coordinates(subdistrict):\n",
    "    try:\n",
    "        location = geolocator.geocode(f\"{subdistrict}, Indonesia\")\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "    except:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# Apply geocoding to get lat/lon for each unique subdistrict\n",
    "tqdm.pandas()\n",
    "df[['latitude', 'longitude']] = df['address_subdistrict'].progress_apply(lambda x: pd.Series(get_coordinates(x)))\n",
    "\n",
    "# Drop rows where coordinates could not be found\n",
    "df.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "\n",
    "# Create a Folium map centered at Indonesia\n",
    "m = folium.Map(location=[-6.1751, 106.8650], zoom_start=10)  # Centered near Jakarta\n",
    "\n",
    "# Add markers to the map\n",
    "for _, row in df.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=f\"{row['address_subdistrict']}\",\n",
    "        icon=folium.Icon(color=\"blue\", icon=\"info-sign\")\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save map to an HTML file\n",
    "m.save(\"subdistrict_map.html\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
